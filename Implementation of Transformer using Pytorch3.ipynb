{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPMGoYRumXH8eGMRnyxPh0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevanshPatel234/FMML_Project_and_Labs/blob/main/Implementation%20of%20Transformer%20using%20Pytorch3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Transformer using Pytorch"
      ],
      "metadata": {
        "id": "Kcho201MhHYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers are a type of deep learning model introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. They have revolutionized natural language processing (NLP) and have been adapted to other domains like computer vision. Here's an overview of the key concepts and components of transformers:\n",
        "\n",
        "Key Concepts\n",
        "\n",
        "Attention Mechanism:\n",
        "\n",
        "Self-Attention: Allows the model to focus on different parts of the input sequence when encoding a particular word, enabling it to capture dependencies regardless of their distance in the sequence.\n",
        "Scaled Dot-Product Attention: The primary mechanism in transformers, computing the attention scores as the dot product of queries (Q) and keys (K), scaled by the square root of the dimension of the keys. The scores are then passed through a softmax function to obtain weights applied to the values (V).\n",
        "\n",
        "Positional Encoding:\n",
        "\n",
        "Since transformers do not have a built-in notion of sequence order (unlike RNNs), positional encodings are added to the input embeddings to retain information about the position of tokens in the sequence.\n",
        "Transformer Architecture\n",
        "\n",
        "Encoder-Decoder Structure:\n",
        "\n",
        "The transformer model is composed of an encoder and a decoder, both built using stacked layers of self-attention and feed-forward neural networks.\n",
        "\n",
        "Encoder:\n",
        "\n",
        "Consists of multiple identical layers, each with two main components:\n",
        "Multi-Head Self-Attention: Enables the model to focus on different positions of the input sequence simultaneously.\n",
        "Feed-Forward Neural Network: Applies a fully connected feed-forward network to each position independently.\n",
        "Residual connections and layer normalization are used around each sub-layer to facilitate training.\n",
        "\n",
        "Decoder:\n",
        "\n",
        "Similar to the encoder but with an additional layer for multi-head attention over the encoder's output.\n",
        "\n",
        "The decoder layers include:\n",
        "Masked Multi-Head Self-Attention: Ensures that the predictions for a position only depend on known outputs up to that position.\n",
        "Multi-Head Attention: Over the encoderâ€™s output to incorporate information from the input sequence.\n",
        "Feed-Forward Neural Network: As in the encoder.\n",
        "\n",
        "Applications\n",
        "\n",
        "Natural Language Processing:\n",
        "\n",
        "Machine Translation: Translating text from one language to another.\n",
        "Text Summarization: Generating concise summaries of long documents.\n",
        "Question Answering: Finding answers to questions from a given context.\n",
        "Computer Vision:\n",
        "\n",
        "Vision Transformers (ViTs): Applying transformer models to image recognition tasks by dividing images into patches and treating them as sequences."
      ],
      "metadata": {
        "id": "fKRL-iqehT39"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6ldsq91ZQr2Q"
      },
      "outputs": [],
      "source": [
        "# Library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MultiHeadAttention"
      ],
      "metadata": {
        "id": "dtuOCCiLiOSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ],
      "metadata": {
        "id": "MDrnSl09Q3Bi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PositionWiseFeedForward"
      ],
      "metadata": {
        "id": "h3v2hBrBiXcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "tAcVgLQVRQ9a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PositionalEncoding"
      ],
      "metadata": {
        "id": "R--9YEQWif5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "Rpl1cTSxRX-n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EncoderLayer"
      ],
      "metadata": {
        "id": "Lhb6F1YLilgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "tR0gr1wbReOn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DecoderLayer"
      ],
      "metadata": {
        "id": "nzzpEV1QirKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "hiy9rtJKRmem"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Model"
      ],
      "metadata": {
        "id": "dMQ5v2pziwCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "e-mhLLuGRu7Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing Sample Data"
      ],
      "metadata": {
        "id": "4FJ2i94oi1c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "# Generate random sample data\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
      ],
      "metadata": {
        "id": "fmiNjg3eR9TB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "cuex7TVljC2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "id": "-tmGmKYsSFxa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0W6b89tSP6-",
        "outputId": "712d3c93-2ce0-461e-d379-416c9d7c9443"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder_embedding): Embedding(5000, 512)\n",
              "  (decoder_embedding): Embedding(5000, 512)\n",
              "  (positional_encoding): PositionalEncoding()\n",
              "  (encoder_layers): ModuleList(\n",
              "    (0-5): 6 x EncoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (decoder_layers): ModuleList(\n",
              "    (0-5): 6 x DecoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (cross_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=512, out_features=5000, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data, tgt_data[:, :-1])\n",
        "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGtwIoXfSW7D",
        "outputId": "4adf5e1b-d8bf-4e85-99cd-44ad7bdf8c55"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 8.677218437194824\n",
            "Epoch: 2, Loss: 8.535113334655762\n",
            "Epoch: 3, Loss: 8.466719627380371\n",
            "Epoch: 4, Loss: 8.414207458496094\n",
            "Epoch: 5, Loss: 8.3574800491333\n",
            "Epoch: 6, Loss: 8.290533065795898\n",
            "Epoch: 7, Loss: 8.211080551147461\n",
            "Epoch: 8, Loss: 8.123992919921875\n",
            "Epoch: 9, Loss: 8.046772956848145\n",
            "Epoch: 10, Loss: 7.965337753295898\n",
            "Epoch: 11, Loss: 7.879910945892334\n",
            "Epoch: 12, Loss: 7.810268402099609\n",
            "Epoch: 13, Loss: 7.719799995422363\n",
            "Epoch: 14, Loss: 7.633733749389648\n",
            "Epoch: 15, Loss: 7.556983470916748\n",
            "Epoch: 16, Loss: 7.469480991363525\n",
            "Epoch: 17, Loss: 7.3881659507751465\n",
            "Epoch: 18, Loss: 7.309781074523926\n",
            "Epoch: 19, Loss: 7.2236409187316895\n",
            "Epoch: 20, Loss: 7.147046089172363\n",
            "Epoch: 21, Loss: 7.063130855560303\n",
            "Epoch: 22, Loss: 6.987451553344727\n",
            "Epoch: 23, Loss: 6.903902530670166\n",
            "Epoch: 24, Loss: 6.831885814666748\n",
            "Epoch: 25, Loss: 6.750979423522949\n",
            "Epoch: 26, Loss: 6.677883625030518\n",
            "Epoch: 27, Loss: 6.604788303375244\n",
            "Epoch: 28, Loss: 6.538572311401367\n",
            "Epoch: 29, Loss: 6.470930099487305\n",
            "Epoch: 30, Loss: 6.399796485900879\n",
            "Epoch: 31, Loss: 6.333756446838379\n",
            "Epoch: 32, Loss: 6.252951145172119\n",
            "Epoch: 33, Loss: 6.186349391937256\n",
            "Epoch: 34, Loss: 6.110637664794922\n",
            "Epoch: 35, Loss: 6.0543365478515625\n",
            "Epoch: 36, Loss: 5.989712715148926\n",
            "Epoch: 37, Loss: 5.918557643890381\n",
            "Epoch: 38, Loss: 5.853349208831787\n",
            "Epoch: 39, Loss: 5.798799991607666\n",
            "Epoch: 40, Loss: 5.734913349151611\n",
            "Epoch: 41, Loss: 5.659931659698486\n",
            "Epoch: 42, Loss: 5.603426933288574\n",
            "Epoch: 43, Loss: 5.540529251098633\n",
            "Epoch: 44, Loss: 5.486820697784424\n",
            "Epoch: 45, Loss: 5.435876846313477\n",
            "Epoch: 46, Loss: 5.375487804412842\n",
            "Epoch: 47, Loss: 5.3136372566223145\n",
            "Epoch: 48, Loss: 5.259395599365234\n",
            "Epoch: 49, Loss: 5.194983959197998\n",
            "Epoch: 50, Loss: 5.135900020599365\n",
            "Epoch: 51, Loss: 5.0830206871032715\n",
            "Epoch: 52, Loss: 5.024711608886719\n",
            "Epoch: 53, Loss: 4.966543197631836\n",
            "Epoch: 54, Loss: 4.9130988121032715\n",
            "Epoch: 55, Loss: 4.857385635375977\n",
            "Epoch: 56, Loss: 4.808592319488525\n",
            "Epoch: 57, Loss: 4.75222110748291\n",
            "Epoch: 58, Loss: 4.697014331817627\n",
            "Epoch: 59, Loss: 4.643731594085693\n",
            "Epoch: 60, Loss: 4.597050666809082\n",
            "Epoch: 61, Loss: 4.542564392089844\n",
            "Epoch: 62, Loss: 4.494805812835693\n",
            "Epoch: 63, Loss: 4.448154926300049\n",
            "Epoch: 64, Loss: 4.391239643096924\n",
            "Epoch: 65, Loss: 4.33920955657959\n",
            "Epoch: 66, Loss: 4.292243957519531\n",
            "Epoch: 67, Loss: 4.243771553039551\n",
            "Epoch: 68, Loss: 4.189316749572754\n",
            "Epoch: 69, Loss: 4.137375354766846\n",
            "Epoch: 70, Loss: 4.086334705352783\n",
            "Epoch: 71, Loss: 4.044466018676758\n",
            "Epoch: 72, Loss: 3.991804361343384\n",
            "Epoch: 73, Loss: 3.945634603500366\n",
            "Epoch: 74, Loss: 3.891613245010376\n",
            "Epoch: 75, Loss: 3.8443758487701416\n",
            "Epoch: 76, Loss: 3.7981202602386475\n",
            "Epoch: 77, Loss: 3.7523446083068848\n",
            "Epoch: 78, Loss: 3.7088370323181152\n",
            "Epoch: 79, Loss: 3.666317939758301\n",
            "Epoch: 80, Loss: 3.615260601043701\n",
            "Epoch: 81, Loss: 3.5599746704101562\n",
            "Epoch: 82, Loss: 3.5160415172576904\n",
            "Epoch: 83, Loss: 3.478116989135742\n",
            "Epoch: 84, Loss: 3.4323863983154297\n",
            "Epoch: 85, Loss: 3.38199782371521\n",
            "Epoch: 86, Loss: 3.336625814437866\n",
            "Epoch: 87, Loss: 3.288999319076538\n",
            "Epoch: 88, Loss: 3.24330472946167\n",
            "Epoch: 89, Loss: 3.2021961212158203\n",
            "Epoch: 90, Loss: 3.158724308013916\n",
            "Epoch: 91, Loss: 3.1202445030212402\n",
            "Epoch: 92, Loss: 3.0717639923095703\n",
            "Epoch: 93, Loss: 3.028512716293335\n",
            "Epoch: 94, Loss: 2.9837985038757324\n",
            "Epoch: 95, Loss: 2.9352102279663086\n",
            "Epoch: 96, Loss: 2.8983335494995117\n",
            "Epoch: 97, Loss: 2.854830265045166\n",
            "Epoch: 98, Loss: 2.809262990951538\n",
            "Epoch: 99, Loss: 2.7684123516082764\n",
            "Epoch: 100, Loss: 2.729099988937378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, src, tgt):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(src, tgt[:, :-1])\n",
        "        _, predicted = torch.max(output, dim=-1)\n",
        "        correct = (predicted == tgt[:, 1:]).sum().item()\n",
        "        total = (tgt[:, 1:] != 0).sum().item()\n",
        "        accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "nJVmE1zygASw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate(transformer, src_data, tgt_data)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGFnzhUagJoz",
        "outputId": "5aae96ec-9b1c-48c2-a922-f7558c93967d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9919507575757576\n"
          ]
        }
      ]
    }
  ]
}